{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script consists of a Python version of Andrew Ng Stanford Course 'Machine Learning' taught on the Coursera Platform\n",
    "Note: All exercise data and structure are credited to Stanford University \n",
    "\n",
    "**Caveat:** Contrary to the modularity presented in Octave scripts and as I'm using Jupyter Notebooks for educational purposes we will implement the functions on the same notebook where we will call them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Generate E-Mail Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy libraries to deal with matrixes and vectors\n",
    "import numpy as np\n",
    "# Import pandas do read data files\n",
    "import pandas as pd\n",
    "# Import matplotlib to plot data\n",
    "import matplotlib.pyplot as plt\n",
    "# Import regular expressions library\n",
    "import re\n",
    "# Import string helper library\n",
    "import string\n",
    "\n",
    "#Import NLTK Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import and load Porter Stemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Import math \n",
    "import math \n",
    "\n",
    "# Import scipy optimization function\n",
    "from scipy import optimize, io\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Import Support Vector Machine\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "# Matplotlib notebook property\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many problems that you can solve with machine learning is the classification of spam e-mails.\n",
    "<br>\n",
    "We will use an SVM to train this classifier. \n",
    "As usual, let's look at the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read e-mail contents\n",
    "file_contents = open(\"emailSample1.txt\", \"r\")\n",
    "file_contents = (file_contents.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to process this text into something readable for the SVM? \n",
    "<br> \n",
    "We need to turn those words into integers of some form - let's start by reading a vocabulary list (this vocab was pre-filtered with only the most common words) after pre-processing and doing some common Natural Language Processing tasks such as:\n",
    "<br>\n",
    "- keeping only alphanumeric characters;\n",
    "- flagging emails or urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabList():\n",
    "    '''\n",
    "    Generates vocabulary list.\n",
    "    Maps string to integer (sti)\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        vocab_dict(dict): Vocabulary_list\n",
    "    '''\n",
    "    vocab_dict = {}\n",
    "    \n",
    "    with open(\"vocab.txt\", \"r\") as vocab:\n",
    "        for line in vocab:\n",
    "            vocab_dict[int((line.split('\\t'))[0]),1] = line.split('\\t')[1].replace('\\n','')\n",
    "            \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmail(\n",
    "    email_contents: str\n",
    ") -> list:\n",
    "    '''\n",
    "    Preprocesses e-mail and returns\n",
    "    word indices according to vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        email_contents(str): Content of the e-mail \n",
    "    Return:\n",
    "        word_indices(list): List of word indexes.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    vocabList = getVocabList()\n",
    "    \n",
    "    word_indices = []\n",
    "\n",
    "    #Lowercase all e-mail contents \n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    #Replace \\n tags\n",
    "    email_contents = email_contents.replace('\\n',' ')\n",
    "\n",
    "    #Regex pattern substitutions\n",
    "    email_contents = re.sub('<[^<>]+>', ' ', email_contents)\n",
    "    \n",
    "    #Replace numbers \n",
    "    email_contents = re.sub('[0-9]+', 'number', email_contents)\n",
    "    \n",
    "    #Handle URL's\n",
    "    email_contents = re.sub('(http|https)://[^\\s]*', 'httpaddr', email_contents)\n",
    "    \n",
    "    #Handle e-mail addresses\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email_contents)\n",
    "    \n",
    "    #Handle $ sign\n",
    "    email_contents = re.sub('[$]+', 'dollar', email_contents)\n",
    "    \n",
    "    email_contents = word_tokenize(email_contents)\n",
    "    \n",
    "    process_email_contents = []\n",
    "    \n",
    "    for el in email_contents:\n",
    "        # Remove punctuation\n",
    "        element = (el.translate(str.maketrans('', '', string.punctuation)))\n",
    "        # Retain only alphanumeric\n",
    "        element = re.sub(r'\\W+', '', element)\n",
    "        if len(element)>=1:\n",
    "            process_email_contents.append(stemmer.stem(element))\n",
    "\n",
    "    # Loop through each element and find corresponding vocab integer value\n",
    "    for el in process_email_contents:\n",
    "        try:\n",
    "            word_indices.append([k for k,v in vocabList.items() if v == el][0][0])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word indices for the process e-mail\n",
    "word_indices = processEmail(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(\n",
    "    word_indices: list\n",
    ") ->np.array:\n",
    "    '''\n",
    "    Returns vectorized version of the e-mail using word \n",
    "    indexes.\n",
    "    Each array element is mapped to an array \n",
    "    consisting of 0's and 1's where 1's are the\n",
    "    presence of the word at index n in the e-mail.\n",
    "    \n",
    "    Args\n",
    "        word_indices(list): List of word indexes\n",
    "    Returns:\n",
    "        vectorized_features(np.array): Word vector.\n",
    "    '''\n",
    "    \n",
    "    vocabList = getVocabList()\n",
    "    \n",
    "    vectorized_features = np.zeros(len(vocabList))\n",
    "    for i in range(0,len(vocabList)):\n",
    "        if i in word_indices:\n",
    "            vectorized_features[i] = 1\n",
    "    \n",
    "    return vectorized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = emailFeatures(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector is 1899\n",
      "Length of non-zero elements is 45.0\n"
     ]
    }
   ],
   "source": [
    "print('Length of feature vector is {}'.format(len(features)))\n",
    "print('Length of non-zero elements is {}'.format(features.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Load Pre-Computed Features and Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scipy Io to load matrix object with exercise data\n",
    "spam_file = io.loadmat('spamTrain.mat')\n",
    "X = np.array(spam_file['X'])\n",
    "y = np.array(spam_file['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-loaded the matrixes for all the spam e-mails using the vocab list above.\n",
    "<br>\n",
    "This matrix object was given by Andrew on his class so we don't need to compute anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As in the first part of exercise 6, we are going to train a Linear SVM and assess the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmTrain(\n",
    "    X: np.array, \n",
    "    y: np.array, \n",
    "    C: float,\n",
    "    max_iter:int\n",
    ") -> SVC:\n",
    "    \n",
    "    '''\n",
    "    Trains a Support Vector Machine Classifier using sklearn\n",
    "    library. \n",
    "    \n",
    "    Args:\n",
    "        X(np.array): Array of original features.\n",
    "        y(np.array): Array of target values.\n",
    "        C(float): Penalty of the Support Vector Machine\n",
    "        max_iter(int): Number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        svm_classifier(sklearn.base.ClassifierMixin): trained\n",
    "        classifier.\n",
    "    '''\n",
    "    \n",
    "    svm_classifier = SVC(C=C, kernel='linear', probability=True)\n",
    "    svm_classifier.fit(X,y.reshape(len(y),))     \n",
    "    \n",
    "    return svm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with a 0.1 penalty\n",
    "C = 0.1\n",
    "model = svmTrain(X,y,C,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict if spam/not spam based on model - we'll use the sklearn predict method \n",
    "p = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 99.825\n"
     ]
    }
   ],
   "source": [
    "print('Model accuracy is {}'.format((p.reshape(len(p),1)==y).sum()/len(y)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy is really high on the training set.\n",
    "<br>\n",
    "Let's check the performance on the test set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scipy Io to load matrix object with exercise test data\n",
    "spam_file = io.loadmat('spamTest.mat')\n",
    "X_test = np.array(spam_file['Xtest'])\n",
    "y_test = np.array(spam_file['ytest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict if spam/not spam based on model - we'll use the sklearn predict method \n",
    "p_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 98.9\n"
     ]
    }
   ],
   "source": [
    "print('Model accuracy is {}'.format((p_test.reshape(len(p_test),1)==y_test).sum()/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model accuracy on the test set is also really good.\n",
    "<br>\n",
    "**Let's take a look at the weight of the features on the algorithm and extract the influence of those features on the target variable.**\n",
    "<br>\n",
    "**Let's look at the top predictors for spam - this is, the words that weigh more on the classification of spam/not spam:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our'}\n",
      "{'click'}\n",
      "{'remov'}\n",
      "{'guarante'}\n",
      "{'visit'}\n",
      "{'basenumb'}\n",
      "{'dollar'}\n",
      "{'will'}\n",
      "{'price'}\n",
      "{'pleas'}\n",
      "{'most'}\n",
      "{'nbsp'}\n",
      "{'lo'}\n",
      "{'ga'}\n",
      "{'hour'}\n"
     ]
    }
   ],
   "source": [
    "vocabList = getVocabList()\n",
    "\n",
    "# Rely on the coefficients of the model to obtain the variable influence\n",
    "weights = model.coef_[0]\n",
    "weights = dict(np.hstack((np.arange(1,1900).reshape(1899,1),weights.reshape(1899,1))))\n",
    "\n",
    "# Sort Weights in Dictionary\n",
    "weights = sorted(weights.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "# Printing the top predictors of spam\n",
    "top_15 = {}\n",
    "for i in weights[:15]:\n",
    "    print({v for k,v in vocabList.items() if k[0] == i[0]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
