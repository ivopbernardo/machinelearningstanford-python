{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script consists of a Python version of Andrew Ng's Stanford Course 'Machine Learning' taught on the Coursera Platform\n",
    "\n",
    "**Note: All exercise data and structure are credited to Stanford University** \n",
    "\n",
    "**Caveat:** Contrary to the modularity presented in Octave scripts and as I'm using Jupyter Notebooks for educational purposes we will implement the functions on the same notebook where we will call them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import numpy libraries to deal with matrixes and vectors\n",
    "import numpy as np\n",
    "#Import pandas do read data files\n",
    "import pandas as pd\n",
    "#Import matplotlib to plot data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Matplotlib notebook property\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First exercise consists of developing a multiple variable linear regression. \n",
    "<br>\n",
    "When we deal with multiple variables it is normally necessary (in most algorithms that rely on distances) to normalize features between a specific range. \n",
    "<br>\n",
    "By doing this, we avoid that the features with a higher range of values (1-1000, as an example) have higher influence on the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Normalizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples: 47 \n",
      "\n",
      "Printing some examples of training table: \n",
      "\n",
      "X = [1600    3] , y = 329900\n",
      "X = [2400    3] , y = 369000\n",
      "X = [1416    2] , y = 232000\n",
      "X = [3000    4] , y = 539900\n",
      "X = [1985    4] , y = 299900\n",
      "X = [1534    3] , y = 314900\n",
      "X = [1427    3] , y = 198999\n",
      "X = [1380    3] , y = 212000\n",
      "X = [1494    3] , y = 242500\n"
     ]
    }
   ],
   "source": [
    "# Read Text File and create X and y variables \n",
    "# Keeping them as Pandas iSeries would be almost the same but \n",
    "# it is more \"pure\" in terms of comparison with Octave to have array objects\n",
    "ex2_file = pd.read_csv('ex1data2.txt', header=None) \n",
    "\n",
    "X = np.array(ex2_file.iloc[:,0:2])\n",
    "\n",
    "y = np.array(ex2_file.iloc[:,2])\n",
    "\n",
    "print('# of training examples: {} \\n'.format(len(X)))\n",
    "\n",
    "print('Printing some examples of training table: \\n')\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    print('X = {} , y = {}'.format(X[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Features - Applying standardization of the variables\n",
    "\n",
    "def featureNormalize(\n",
    "    features: np.array\n",
    "):\n",
    "    mean = X.mean(axis=0)\n",
    "    # To get the same result as Octave std function you have to set degrees of freedom to 1 with the ddof parameter\n",
    "    sigma = X.std(axis=0,ddof=1)\n",
    "    X_norm = ((X-mean)/sigma)\n",
    "    return X_norm, mean, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are going to apply a standardization of the variables, based on the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/Standardization.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where Xi is equal to the ith observation,\n",
    "<br>\n",
    "μ is equal to the mean of the variable and \n",
    "<br>\n",
    "σ is equal to the standard deviation of the variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve original matrix, mu and sigma\n",
    "X, mu, sigma = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of ones to the array \n",
    "ones_vector = np.ones((len(X),1))\n",
    "# Creating X_ext object to avoid overwriting original X object\n",
    "X_ext = np.hstack((ones_vector,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we have done with a single variable, we are going to do gradient descent for multiple variables.\n",
    "<br>\n",
    "The difference in this example is that we are going to have a new Θ to optimize (Θ2).\n",
    "\n",
    "We are going to do a vectorized approach as, for larger values of m (observations) or n (number of Θ's to optimize), a vectorized approach is always preferable as it is computationally cheaper.\n",
    "\n",
    "Checking the formulas: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/Multivariate Cost Function.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient for multiple features \n",
    "# Initialize theta, learning rates and number of iterations\n",
    "theta = np.zeros((3,1))\n",
    "alpha = 0.1;\n",
    "num_iters = 400;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multivariable Cost Function\n",
    "def computeCostMulti (\n",
    "    X: np.array, \n",
    "    y:np.array , \n",
    "    theta: np.array\n",
    ") -> int:\n",
    "    m = len(y) \n",
    "    # Compute hypothesis\n",
    "    hypothesis = np.dot(X,theta)\n",
    "    # Compute cost function, using average squared error due\n",
    "    J = (sum((hypothesis-y.reshape(len(y),1))**2))/(2*m)\n",
    "    return J\n",
    "\n",
    "# Define multivariate Gradient Descent\n",
    "\n",
    "def gradientDescentMulti(\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    theta: np.array,\n",
    "    alpha: float,\n",
    "    num_iters: int\n",
    ") -> [np.array, np.array]:\n",
    "    \n",
    "    m = len(y)\n",
    "    # Create vector where we will input each result of the cost function\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    # loop through each iteration\n",
    "    for i in np.arange(0,num_iters):\n",
    "        # Compute hypothesis\n",
    "        hypothesis = np.dot(X,theta)\n",
    "        # Update theta by computing gradient\n",
    "        theta -= alpha * (1/m) * np.dot(np.transpose(X),(hypothesis-y.reshape(len(y),1)))\n",
    "        # Assign cost function value with new theta to the J_history ith position\n",
    "        J_history[i] = computeCostMulti(X,y,theta)\n",
    "    return theta, J_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return value of theta on the last iteration and an array with the history of the cost function\n",
    "theta, J_history = gradientDescentMulti(X_ext,y,theta,alpha,num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the convergence of the Cost Function we can notice several things:\n",
    "<br>\n",
    "    -  The cost function stabilized after ~300 iterations;\n",
    "    -  The cost function changes by a higher magnitude on the first iterations. After the 250th iteration, the change in the Θ parameters only bring decimal changes to the cost function;\n",
    "<br>\n",
    "We will use the code below to plot the J_history object - Cost Function at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost Function (J)')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAERCAYAAABxZrw0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAasUlEQVR4nO3de7QddX338fcnJzk5gYQEwoECCUQiiinKxQOo+CDkcSFaKmpBtGB1ySq9KEVtdcmyT6XSZUEeWLRe+pgil1bExyIsBJWIEKBVgZwECMlDuQkIcsmBhCTcQi7f54/fbM8+t33mXGbPzuzPa61Zs/fs2TPfDIfP+Z3fzPxGEYGZmVXPlLILMDOzYjjgzcwqygFvZlZRDngzs4pywJuZVZQD3sysolou4CVdKmmtpNU51j1a0kpJWyWdNOizj0t6KJs+XlzFZmatqeUCHrgcOD7nur8BPgF8r36hpN2ALwNHAkcAX5a06+SVaGbW+lou4CPidmBd/TJJCyXdKGmFpP+UdGC27mMRsQrYPmgz7wFuioh1EbEeuIn8vzTMzCphatkF5LQE+POIeEjSkcC3gMUN1t8HeKLu/ZPZMjOzttHyAS9pJvAO4D8k1RZPH+1rwyzzmAxm1lZaPuBJ3UgvRMQhY/jOk8Axde/nAbdOYk1mZi2v5frgB4uIjcCjkk4GUHLwKF9bChwnadfs5Opx2TIzs7bRcgEv6SrgV8AbJT0p6XTgVOB0SfcCa4ATs3UPl/QkcDLwbUlrACJiHXAusDybvpItMzNrG/JwwWZm1dRyLXgzM5scLXWSdffdd48FCxaUXYaZ2Q5jxYoVz0VE93CftVTAL1iwgN7e3rLLMDPbYUh6fKTP3EVjZlZRDngzs4pywJuZVZQD3sysohzwZmYV5YA3M6soB7yZWUVVIuDPPReWeigxM7MBKhHw550HN91UdhVmZq2lEgE/bRps2VJ2FWZmrcUBb2ZWUZUI+M5OeO21sqswM2stlQh4t+DNzIZywJuZVZQD3sysohzwZmYV5YA3M6soB7yZWUUV+sg+SY8Bm4BtwNaI6CliPw54M7OhmvFM1mMj4rkid9DZCa+8UuQezMx2PO6iMTOrqKIDPoCfSVoh6YzhVpB0hqReSb19fX3j2okD3sxsqKID/qiIOAx4L/ApSUcPXiEilkRET0T0dHd3j2snDngzs6EKDfiIeCqbrwWuBY4oYj8OeDOzoQoLeEk7S5pVew0cB6wuYl8OeDOzoYq8imZP4FpJtf18LyJuLGJHDngzs6EKC/iI+DVwcFHbr+eANzMbqhKXSXo8eDOzoSoR8G7Bm5kN5YA3M6soB7yZWUVVJuC3b0+TmZkllQl4cCvezKyeA97MrKIc8GZmFVWJgO/sTHNfC29m1q8SAe8WvJnZUA54M7OKcsCbmVWUA97MrKIc8GZmFeWANzOrqEoFvC+TNDPrV4mAr10H7xa8mVm/SgS8u2jMzIZywJuZVVQlAt5DFZiZDVWpgHcL3sysX6UCfvPmcuswM2sllQj46dPT3F00Zmb9KhHwbsGbmQ1ViYB3C97MbKhKBLxb8GZmQ1Uq4N2CNzPrV4mAnzoVJAe8mVm9SgS8lFrx7qIxM+tXeMBL6pB0t6QbitzP9OluwZuZ1WtGC/4s4P6id+IWvJnZQIUGvKR5wB8AlxS5H0gB7xa8mVm/olvwFwNfALYXvB930ZiZDVJYwEs6AVgbEStGWe8MSb2Sevv6+sa9P3fRmJkNVGQL/ijg/ZIeA74PLJb03cErRcSSiOiJiJ7u7u5x78wteDOzgQoL+Ig4OyLmRcQC4CPALRFxWlH7cwvezGygSlwHDz7JamY22NRm7CQibgVuLXIf7qIxMxuoUi14d9GYmfWrTMC7BW9mNlBlAt4teDOzgSoV8G7Bm5n1a3iSVVIXcALwP4C9gVeA1cCPI2JN8eXlN326W/BmZvVGDHhJ5wB/SLr65U5gLdAFvAE4Lwv/v46IVcWXOTq34M3MBmrUgl8eEeeM8NlFkvYA9p38ksbHJ1nNzAYaMeAj4seNvhgRa0mt+pbgk6xmZgM16qK5HogRPt4MPAJ8MyKeKKKwsXIL3sxsoEZdNP97lO/9PvAD4O2TWtE4dXbCtm1p6ugouxozs/I16qK5bZTv3izpLZNcz7h1dqb5a6/BjBnl1mJm1gpGvA5e0vWS/lDStGE+21/SV4BfFlrdGEyfnubuhzczSxp10fwp8DngYknrgD7SZZILSP3v34iI6wqvMCcHvJnZQI26aJ4hPW7vC5IWAHuRbnR6MCJebkp1Y9DVleavvlpuHWZmrSLXcMER8RjwWKGVTJAD3sxsoMqMRVMLeHfRmJkllQt4t+DNzBIHvJlZRY3aBy/pKOAcYL9sfQEREfsXW9rYOODNzAbKc5L1O8BngRXAtmLLGT8HvJnZQHkCfkNE/LTwSibIAW9mNlCegF8m6QLgGtIgYwBExMrCqhoHB7yZ2UB5Av7IbN5TtyyAxZNfzvjV7mR1wJuZJaMGfEQc24xCJsoteDOzgUa9TFLSbEkXSerNpgslzW5GcWPhgDczGyjPdfCXApuAD2fTRuCyIosaD3fRmJkNlKcPfmFE/FHd+7+XdE9RBY3X1KlpcsCbmSV5WvCvSHpn7U1249MrxZU0fl1dDngzs5o8Lfi/AK7I+t0FrAM+UWRR4+WANzPrl+cqmnuAgyXtkr3fWHhV49TV5dEkzcxqRgx4SadFxHclfW7QcgAi4qJGG5bUBdwOTM/2c3VEfHnCFTfgFryZWb9GLfids/msYT6LHNveDCyOiBez57r+l6SfRsQdYy0yLwe8mVm/Ro/s+3b28ucR8Yv6z7ITrQ1FRAAvZm+nZVOeXwzj5oA3M+uX5yqar+dcNoSkjuySyrXATRFx5zDrnFG7iaqvry/PZkfkgDcz69eoD/7twDuA7kH98LsAHXk2HhHbgEMkzQGulXRQRKwetM4SYAlAT0/PhFr4XV3wcss9DtzMrByNWvCdwEzSL4FZddNG4KSx7CQiXgBuBY4fV5U5uQVvZtavUR/8bcBtki6PiMfHumFJ3cCWiHhB0gzg3cD54y91dF1d8EpL3oJlZtZ8efrgL8m6WACQtKukpTm+txdpLPlVwHJSH/wN46wzlxkzHPBmZjV57mTdPetiASAi1kvaY7QvRcQq4NCJFDdWO+3kPngzs5o8LfjtkvatvZG0HwVf7jheDngzs355WvBfIt2kdFv2/mjgjOJKGr9awEdAdsOtmVnbyjMWzY2SDgPeRhps7LMR8VzhlY3DTjvB9u2wZQt0dpZdjZlZufJ00UAaT2YdsAFYJOno4koavxkz0tzdNGZmOVrwks4HTgHWANuzxUEaSKyl7LRTmr/8MsyZ03hdM7Oqy9MH/wHgjRHR8gPx1ge8mVm7y9NF82vSQGEtzwFvZtYvTwv+ZeAeSTeThgAGICL+qrCqxskBb2bWL0/A/yibWl4t4H03q5lZvsskr2hGIZPBLXgzs355rqJ5lGHuXI2I/QupaAJ8maSZWb88XTQ9da+7gJOB3YopZ2Lcgjcz6zfqVTQR8Xzd9NuIuBhY3ITaxswBb2bWL08XzWF1b6eQWvTDPYi7dA54M7N+ebpoLqx7vRV4FPhwMeVMjPvgzcz6NXom69si4o6IOLaZBU3EtGlp8mWSZmaN++C/VXsh6VdNqGVSzJjhFryZGTQO+PoR1buKLmSy7LwzvPRS2VWYmZWvUR/8FEm7kn4J1F7/LvQjYl3RxY3HzJnw4otlV2FmVr5GAT8bWEF/qK+s+yyAlrvRCRzwZmY1IwZ8RCxoYh2TxgFvZpbkfaLTDmPWLNi0qewqzMzKV7mAdwvezCxxwJuZVdSoAS/p3/MsaxWzZjngzcwgXwv+9+vfSOoA3lpMORNXa8HHkAGOzczay4gBL+lsSZuAt0jamE2bgLXAdU2rcIxmzoRt2+DVV8uuxMysXCMGfET8Y0TMAi6IiF2yaVZEzI2Is5tY45jMnJnm7qYxs3aXp4vmBkk7A0g6TdJFkvYruK5xm5UNZOyAN7N2lyfg/wV4WdLBwBeAx4F/G+1LkuZLWibpfklrJJ01wVpzcQvezCzJE/BbIyKAE4F/ioh/It8DP7YCfx0RbwLeBnxK0qLxl5qPA97MLMkT8JsknQ18DPhxdhXNtNG+FBFPR8TK7PUm4H5gn4kUm0ct4H03q5m1uzwBfwqwGfhkRDxDCukLxrITSQuAQ4E7x1jfmLkP3swsyfPQ7WeAK4HZkk4AXo2IUfvgayTNBH4IfCYiNg7z+RmSeiX19vX1jaH04dVa8BuH7MnMrL3kuZP1w8BdwMmkZ7HeKemkPBuXNI0U7ldGxDXDrRMRSyKiJyJ6uru781c+gtmz09wBb2btLs9Dt78EHB4RawEkdQM/B65u9CVJAr4D3B8RF0200Lx22SXNX3ihWXs0M2tNefrgp9TCPfN8zu8dRToxu1jSPdn0vvEUORbTpqXH9m3YUPSezMxaW54W/I2SlgJXZe9PAX462pci4r8Y+FzXppk92wFvZjZqwEfE5yV9CHgnKbCXRMS1hVc2AQ54M7MGAS/p9cCeEfGL7ATpNdnyoyUtjIhHmlXkWDngzcwa96VfDAx3u9DL2WctywFvZtY44BdExKrBCyOiF1hQWEWTwAFvZtY44LsafDZjsguZTLNn+zJJM7NGAb9c0p8OXijpdGBFcSVNnFvwZmaNr6L5DHCtpFPpD/QeoBP4YNGFTcTs2emJTq+9Bp2dZVdjZlaOEQM+Ip4F3iHpWOCgbPGPI+KWplQ2AXPmpPmGDTAJox+Yme2Q8lwHvwxY1oRaJk0t4Nevd8CbWfvKM+TADmfu3DRft67cOszMylTJgN9ttzR//vly6zAzK1MlA77WgnfAm1k7q3TAu4vGzNpZJQN+9myYMsUteDNrb5UM+ClTUj+8A97M2lklAx4c8GZmlQ34uXPdB29m7a3SAe8WvJm1s0oH/HPPlV2FmVl5Khvwe+wBa9dCRNmVmJmVo7IBv+eesHkzbNxYdiVmZuWobMD/3u+l+TPPlFuHmVlZKhvwe+6Z5s8+W24dZmZlccCbmVWUA97MrKIqG/Bz56YhCxzwZtauKhvwHR3paU4+yWpm7aqyAQ+w997w1FNlV2FmVo5KB/y8efDEE2VXYWZWjkoH/Pz58OSTZVdhZlaOwgJe0qWS1kpaXdQ+RjN/PqxfDy+9VFYFZmblKbIFfzlwfIHbH9X8+Wnubhoza0eFBXxE3A6UOiL7vHlp7m4aM2tHpffBSzpDUq+k3r6+vknddq0F/5vfTOpmzcx2CKUHfEQsiYieiOjp7u6e1G3Pm5euh3/00UndrJnZDqH0gC9SZyfsuy888kjZlZiZNV+lAx5g4UJ4+OGyqzAza74iL5O8CvgV8EZJT0o6vah9NfL617sFb2btaWpRG46Ijxa17bFYuBDWrUvXw++6a9nVmJk1T+W7aA44IM0ffLDcOszMmq3yAb9oUZqvWVNuHWZmzVb5gN9/f+jqcsCbWfupfMB3dMCb3gSrSxsRx8ysHJUPeICDDnLAm1n7aYuAP/TQ9OCPp58uuxIzs+Zpi4A/4og0X7683DrMzJqpLQL+0ENTX7wD3szaSVsE/E47wZvfDL/8ZdmVmJk1T1sEPMAxx6SAf/XVsisxM2uOtgn4xYtTuN9xR9mVmJk1R9sE/NFHp374pUvLrsTMrDnaJuBnz07dNNdeW3YlZmbN0TYBD/ChD8EDD3jYAjNrD20V8CedBFOnwmWXlV2JmVnx2irg99gD3v9+uOIKeOWVsqsxMytWWwU8wJlnwnPPuRVvZtXXdgH/rnfBUUfBP/wDbNpUdjVmZsVpu4CX4MIL08Bjf/u3ZVdjZlactgt4gCOPTF01//zPcNVVZVdjZlaMwh663eq+9jVYtQo+9jGYNi1dYWNmViVt2YKH9Bi/66+Hww+Hk09OLfoNG8quysxs8rRtwAPMmgW33AJnnQXf+Abstx98/vNpWOFt28quzsxsYhQRZdfwOz09PdHb21vKvleuhK9+Fa67DrZuhZkzoacHFi2C+fNh333TdfSzZqVpl13SOp2dqYtn6tR0AtfMrJkkrYiInuE+a9s++MEOOwyuvhrWrYOf/CSNOrl8eToJu359vm10dPSHfW3e0ZE+k/qnwe+HWzbaOlYsH+Ni+fgONHcu3H775G/XAT/IbrvBaaelqeall+CJJ9INUps2wcaNadq0CbZsSdPWrcPPt2+HiP4JBr4fbtlo61ixfIyL5eM71Jw5xWzXAZ/DzjvDgQeWXYWZ2di09UlWM7Mqc8CbmVWUA97MrKIKDXhJx0t6QNLDkr5Y5L7MzGygwgJeUgfwTeC9wCLgo5IWFbU/MzMbqMgW/BHAwxHx64h4Dfg+cGKB+zMzszpFBvw+wBN175/Mlg0g6QxJvZJ6+/r6CizHzKy9FBnww92rNuQWh4hYEhE9EdHT3d1dYDlmZu2lyBudngTm172fBzzV6AsrVqx4TtLj49zf7sBz4/xukVzX2LiusWnVuqB1a6taXfuN9EFhg41Jmgo8CPxP4LfAcuCPI2JNQfvrHWnAnTK5rrFxXWPTqnVB69bWTnUV1oKPiK2SPg0sBTqAS4sKdzMzG6rQsWgi4ifAT4rch5mZDa9Kd7IuKbuAEbiusXFdY9OqdUHr1tY2dbXUAz/MzGzyVKkFb2ZmdRzwZmYVtcMHfCsNaCbpMUn3SbpHUm+2bDdJN0l6KJvv2qRaLpW0VtLqumUj1iLp7OwYPiDpPU2u6xxJv82O2z2S3ldCXfMlLZN0v6Q1ks7Klpd6zBrUVeoxk9Ql6S5J92Z1/X22vOzjNVJdpf+MZfvqkHS3pBuy98Uer4jYYSfS5ZePAPsDncC9wKIS63kM2H3Qsq8BX8xefxE4v0m1HA0cBqwerRbSYHD3AtOB12XHtKOJdZ0D/M0w6zazrr2Aw7LXs0j3cCwq+5g1qKvUY0a6U31m9noacCfwthY4XiPVVfrPWLa/zwHfA27I3hd6vHb0FvyOMKDZicAV2esrgA80Y6cRcTuwLmctJwLfj4jNEfEo8DDp2DarrpE0s66nI2Jl9noTcD9p7KRSj1mDukbSrLoiIl7M3k7LpqD84zVSXSNp2s+YpHnAHwCXDNp/YcdrRw/4XAOaNVEAP5O0QtIZ2bI9I+JpSP+zAnuUVt3ItbTCcfy0pFVZF07tz9RS6pK0ADiU1PprmWM2qC4o+Zhl3Q33AGuBmyKiJY7XCHVB+T9jFwNfALbXLSv0eO3oAZ9rQLMmOioiDiONgf8pSUeXWMtYlH0c/wVYCBwCPA1cmC1vel2SZgI/BD4TERsbrTrMssJqG6au0o9ZRGyLiENI40wdIemgBquXXVepx0vSCcDaiFiR9yvDLBtzXTt6wI95QLMiRcRT2XwtcC3pT6pnJe0FkM3XllVfg1pKPY4R8Wz2P+V24F/p/1O0qXVJmkYK0Ssj4ppscenHbLi6WuWYZbW8ANwKHE8LHK/h6mqB43UU8H5Jj5G6khdL+i4FH68dPeCXAwdIep2kTuAjwI/KKETSzpJm1V4DxwGrs3o+nq32ceC6MurLjFTLj4CPSJou6XXAAcBdzSqq9gOe+SDpuDW1LkkCvgPcHxEX1X1U6jEbqa6yj5mkbklzstczgHcD/035x2vYuso+XhFxdkTMi4gFpJy6JSJOo+jjVdTZ4mZNwPtIVxY8AnypxDr2J531vhdYU6sFmAvcDDyUzXdrUj1Xkf4U3UJqDZzeqBbgS9kxfAB4b5Pr+nfgPmBV9oO9Vwl1vZP0J/Aq4J5sel/Zx6xBXaUeM+AtwN3Z/lcDfzfaz3vJdZX+M1a3v2Pov4qm0OPloQrMzCpqR++iMTOzETjgzcwqygFvZlZRDngzs4pywJuZVZQD3gojKSRdWPf+bySdM0nbvlzSSZOxrVH2c7LSSI7LBi1foGxETEmH1I9OOAn7nCPpL+ve7y3p6snavrUPB7wVaTPwIUm7l11IPUkdY1j9dOAvI+LYBuscQro2fSw1NHoe8hzgdwEfEU9FROG/zKx6HPBWpK2k50x+dvAHg1vgkl7M5sdIuk3SDyQ9KOk8SadmY3zfJ2lh3WbeLek/s/VOyL7fIekCScuzgaX+rG67yyR9j3TDy+B6Ppptf7Wk87Nlf0e60ej/SLpguH9gdgf1V4BTlMYZPyW7q/nSrIa7JZ2YrfsJSf8h6XrSoHQzJd0saWW279pIqOcBC7PtXTDor4UuSZdl698t6di6bV8j6UalscW/Vnc8Ls/+XfdJGvLfwqqrUSvCbDJ8E1hVC5ycDgbeRBpW+NfAJRFxhNLDLs4EPpOttwB4F2kQqWWSXg/8CbAhIg6XNB34haSfZesfARwUafjV35G0N3A+8FZgPSl8PxARX5G0mDSOeO9whUbEa9kvgp6I+HS2va+SbkX/ZHbb/F2Sfp595e3AWyJiXdaK/2BEbMz+yrlD0o9I44IfFGnArNookjWfyvb7ZkkHZrW+IfvsENJok5uBByR9nTQ64T4RcVC2rTmND71ViVvwVqhIIx/+G/BXY/ja8kjjoG8m3apdC+j7SKFe84OI2B4RD5F+ERxIGgPoT5SGi72TdCv4Adn6dw0O98zhwK0R0RcRW4ErSQ8mGa/jgC9mNdwKdAH7Zp/dFBG18fAFfFXSKuDnpOFg9xxl2+8k3XZPRPw38DhQC/ibI2JDRLwK/D9gP9Jx2V/S1yUdDzQaIdMqxi14a4aLgZXAZXXLtpI1MLIBtTrrPttc93p73fvtDPyZHTzORpBC88yIWFr/gaRjgJdGqG+4oVknQsAfRcQDg2o4clANpwLdwFsjYovSSINdObY9kvrjtg2YGhHrJR0MvIfU+v8w8Mlc/wrb4bkFb4XLWqw/IJ2wrHmM1CUC6ek108ax6ZMlTcn65fcnDcq0FPgLpSF2kfQGpdE9G7kTeJek3bMTsB8FbhtDHZtIj9OrWQqcmf3iQtKhI3xvNmmM8C1ZX/p+I2yv3u2kXwxkXTP7kv7dw8q6fqZExA+B/0V6XKK1CQe8NcuFQP3VNP9KCtW7gMEt27weIAXxT4E/z7omLiF1T6zMTkx+m1H+Uo30JJ2zgWWk0UBXRsRYhnVeBiyqnWQFziX9wlqV1XDuCN+7EuhRekD7qaThdomI50nnDlYPc3L3W0CHpPuA/wt8IuvKGsk+wK1Zd9Hl2b/T2oRHkzQzqyi34M3MKsoBb2ZWUQ54M7OKcsCbmVWUA97MrKIc8GZmFeWANzOrqP8PtHtKscKPN1kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the convergence graph - notice how after 250 iterations the curve starts to become flat\n",
    "plt.plot(np.arange(0,num_iters),J_history,c='blue')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Cost Function (J)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from gradient descent \n",
      " [[340412.65957447]\n",
      " [110631.04895815]\n",
      " [ -6649.47295013]]\n"
     ]
    }
   ],
   "source": [
    "print('Theta computed from gradient descent \\n {}'.format(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = np.array((1, 1650, 3)).astype('float64')\n",
    "test_example[1:] = (test_example[1:].reshape(1,2)-mu.reshape(1,2))/sigma.reshape(1,2)\n",
    "price = np.dot(test_example, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for a 1650 sq-ft, 3 br house (using gradient descent) is : \n",
      " 293081\n"
     ]
    }
   ],
   "source": [
    "print ('Predicted price for a 1650 sq-ft, 3 br house (using gradient descent) is : \\n {}'.format(int(price)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for this linear problem we have used gradient descent to minimize the squared error function. \n",
    "<br> \n",
    "But, there is another method that can be used when finding the minimum error for multiple linear regression (also, this method ends up being less computationally expensive as it solves the problem by using vectorization, avoiding the gradient descent iteration).\n",
    "That method is called Normal Equations or OLS - Ordinary least squares.\n",
    "\n",
    "The vectorized formula is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/NormalEquations.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data Again\n",
    "ex2_file = pd.read_csv('ex1data2.txt', header=None) \n",
    "X = np.array(ex2_file.iloc[:,0:2])\n",
    "y = np.array(ex2_file.iloc[:,2])\n",
    "\n",
    "m = y.size\n",
    "\n",
    "#Add column of ones to the array \n",
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(\n",
    "    X: np.array,\n",
    "    y: np.array\n",
    ") -> np.array:\n",
    "    \n",
    "    # Calculate inverse of Matrix\n",
    "    \n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = normalEqn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from the Normal Equation Method \n",
      " [340412.65957447 110631.05027885  -6649.47427082]\n"
     ]
    }
   ],
   "source": [
    "print('Theta computed from the Normal Equation Method \\n {}'.format(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for a 1650 sq-ft, 3 br house (using normal equation) is : \n",
      " 293081 $\n"
     ]
    }
   ],
   "source": [
    "test_example = np.array((1, 1650, 3)).astype('float64')\n",
    "test_example[1:] = (test_example[1:].reshape(1,2)-mu.reshape(1,2))/sigma.reshape(1,2)\n",
    "price = np.dot(test_example, theta)\n",
    "\n",
    "print ('Predicted price for a 1650 sq-ft, 3 br house (using normal equation) is : \\n {} $'.format(int(price)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
