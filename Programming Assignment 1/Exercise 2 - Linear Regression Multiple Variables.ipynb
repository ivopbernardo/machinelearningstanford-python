{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script consists of a Python version of Andrew Ng's Stanford Course 'Machine Learning' taught on the Coursera Platform\n",
    "\n",
    "**Note: All exercise data and structure are credited to Stanford University** \n",
    "\n",
    "**Caveat:** Contrary to the modularity presented in Octave scripts and as I'm using Jupyter Notebooks for educational purposes we will implement the functions on the same notebook where we will call them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy libraries to deal with matrixes and vectors\n",
    "import numpy as np\n",
    "# Import pandas do read data files\n",
    "import pandas as pd\n",
    "# Import matplotlib to plot data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Matplotlib notebook property\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First exercise consists of developing a multiple variable linear regression. \n",
    "<br>\n",
    "<br>\n",
    "When we deal with multiple variables it is normally necessary (in most algorithms that rely on distances) to normalize features between a specific range. In gradient descent, normalizing features enables the algorithm to converge much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Normalizing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are going to analyze is part of a real estate company. In this example, we are trying to predict the value of the house based on two variables:\n",
    "- The size of the house (in meters squares);\n",
    "- The number of bedrooms;\n",
    "<br>\n",
    "<br>\n",
    "Can we develop an algorithm capable of, given the size and bedroom number, predict the value of the estate with accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples: 47 \n",
      "\n",
      "Printing some examples of training table: \n",
      "\n",
      "X = [1600    3] , y = 329900\n",
      "X = [2400    3] , y = 369000\n",
      "X = [1416    2] , y = 232000\n",
      "X = [3000    4] , y = 539900\n",
      "X = [1985    4] , y = 299900\n",
      "X = [1534    3] , y = 314900\n",
      "X = [1427    3] , y = 198999\n",
      "X = [1380    3] , y = 212000\n",
      "X = [1494    3] , y = 242500\n"
     ]
    }
   ],
   "source": [
    "# Read Text File and create X and y variables \n",
    "ex2_file = pd.read_csv('ex1data2.txt', header=None) \n",
    "\n",
    "X = np.array(ex2_file.iloc[:,0:2])\n",
    "\n",
    "y = np.array(ex2_file.iloc[:,2])\n",
    "\n",
    "print('# of training examples: {} \\n'.format(len(X)))\n",
    "\n",
    "print('Printing some examples of training table: \\n')\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    print('X = {} , y = {}'.format(X[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Features - Applying standardization of the variables\n",
    "\n",
    "def featureNormalize(\n",
    "    features: np.array\n",
    ") -> [np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns the standardized version of matrix \"features\" \n",
    "    The formula applied is z = ((xi-mean)/standard deviation)\n",
    "    where i is each observation for nth column\n",
    "    \n",
    "    Args:\n",
    "        features (np.array): The original nth column matrix to normalize.\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (np.array): Matrix of normalized values.\n",
    "        mean (np.array): Mean of each nth column.\n",
    "        sigma (np.array): Standard deviation of each nth column.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    mean = features.mean(axis=0)\n",
    "    # To get the same result as Octave std function you have to set degrees of freedom to 1 with the ddof parameter\n",
    "    sigma = features.std(axis=0,ddof=1)\n",
    "    X_norm = ((features-mean)/sigma)\n",
    "    return X_norm, mean, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply a standardization of the variables, based on the following formula for each variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/Standardization.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The standardization of variable z is done by applying the formula above, where:**\n",
    "<br>\n",
    "- Xi is equal to the ith observation\n",
    "<br>\n",
    "- μ is equal to the mean of the variable \n",
    "<br>\n",
    "- σ is equal to the standard deviation of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve normalized matrix, mu and sigma\n",
    "X, mu, sigma = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of ones to the array \n",
    "ones_vector = np.ones((len(X),1))\n",
    "# Creating X_ext object to avoid overwriting original X object\n",
    "X_ext = np.hstack((ones_vector,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we have done with a single variable, we are going to do gradient descent for multiple variables.\n",
    "<br>\n",
    "The difference for in this example is that we are going to have a new Θ to optimize (Θ2) - rather than just Θ0 and Θ1.\n",
    "\n",
    "We are going to do a vectorized approach as, for larger values of m (observations) or n (number of Θ's to optimize), a vectorized approach is always preferable than a loop approach as it is computationally cheaper.\n",
    "\n",
    "Checking the formulas: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/Multivariate Cost Function.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient for multiple features \n",
    "# Initialize theta, learning rates and number of iterations\n",
    "theta = np.zeros((3,1))\n",
    "alpha = 0.1;\n",
    "num_iters = 400;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multivariable Cost Function\n",
    "def computeCostMulti (\n",
    "    X: np.array, \n",
    "    y:np.array , \n",
    "    theta: np.array\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns the cost function (average squared error) \n",
    "    using matrix X*Theta and it's output Y\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): The values of each observation.\n",
    "        y (np.array): The values to predict.\n",
    "        theta (np.array): The weight to give to each variable that \n",
    "        will be multiplied by X to represent the hypothesis.\n",
    "        \n",
    "    Returns:\n",
    "        J (int): The sum of the errors - overall cost function for \n",
    "        the presented thetas.\n",
    "        \n",
    "    \"\"\" \n",
    "    m = len(y) \n",
    "    # Compute hypothesis\n",
    "    hypothesis = np.dot(X,theta)\n",
    "    # Compute cost function, using average squared error\n",
    "    J = (sum((hypothesis-y.reshape(len(y),1))**2))/(2*m)\n",
    "    return J\n",
    "\n",
    "# Define multivariate Gradient Descent\n",
    "def gradientDescentMulti(\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    theta: np.array,\n",
    "    alpha: float,\n",
    "    num_iters: int\n",
    ") -> [np.array, np.array]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes new theta based on iterations \n",
    "    and history of cost functions for each \n",
    "    iteration of gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): The values of each observation.\n",
    "        y (np.array): The values to predict.\n",
    "        theta (np.array): The initial weight to give to each variable that \n",
    "        will be multiplied by X to represent the hypothesis.\n",
    "        alpha(float): The learning rate of gradient descent.\n",
    "        num_iters(int): The number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        Theta (np.array): The array of optimized thetas returned by \n",
    "        the final iteration of gradiend descent.\n",
    "        J_history (np.array): The history of the error function (J) \n",
    "        during every iteration of gradient descent.\n",
    "        \n",
    "    \"\"\" \n",
    "    \n",
    "    m = len(y)\n",
    "    # Create vector where we will input each result of the cost function\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    # loop through each iteration\n",
    "    for i in np.arange(0,num_iters):\n",
    "        # Compute hypothesis\n",
    "        hypothesis = np.dot(X,theta)\n",
    "        # Update theta by computing gradient\n",
    "        theta -= alpha * (1/m) * np.dot(np.transpose(X),(hypothesis-y.reshape(len(y),1)))\n",
    "        # Assign cost function value with new theta to the J_history ith position\n",
    "        J_history[i] = computeCostMulti(X,y,theta)\n",
    "    return theta, J_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return value of theta on the last iteration and an array with the history of the cost function\n",
    "theta, J_history = gradientDescentMulti(X_ext,y,theta,alpha,num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the convergence of the Cost Function we can notice several things:\n",
    "<br>\n",
    "    -  The cost function stabilized after ~25 iterations;\n",
    "    -  The cost function changes by a higher magnitude on the first iterations. After the 25th iteration, the change in the Θ parameters only bring decimal changes to the cost function J;\n",
    "<br>\n",
    "We will use the code below to plot the J_history object - Cost Function at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost Function (J)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the convergence graph - notice how after 25 iterations the curve starts to become flat\n",
    "plt.plot(np.arange(0,num_iters),J_history,c='blue')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Cost Function (J)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can make the cost function coverge slower by using a lower learning rate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What were the optimal thetas created?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from gradient descent \n",
      " [[340412.65957447]\n",
      " [110631.04895815]\n",
      " [ -6649.47295013]]\n"
     ]
    }
   ],
   "source": [
    "print('Theta computed from gradient descent \\n {}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the new thetas to predict new examples of data coming in. For example, imagine we have a new house that is 1650 square feet and has three bedrooms. Given this, what is the predicted price for this house?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = np.array((1, 1650, 3)).astype('float64')\n",
    "test_example[1:] = (test_example[1:].reshape(1,2)-mu.reshape(1,2))/sigma.reshape(1,2)\n",
    "price = np.dot(test_example, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for a 1650 sq-ft, 3 br house (using gradient descent) is : \n",
      " 293081 $\n"
     ]
    }
   ],
   "source": [
    "print ('Predicted price for a 1650 sq-ft, 3 br house (using gradient descent) is : \\n {} $'.format(int(price)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this linear problem we have used gradient descent to minimize the squared error function and thus optimize our thetas.\n",
    "<br>\n",
    "<br> \n",
    "But, there is another method that can be used when finding the minimum error for multiple linear regression (also, this method ends up being computationally cheaper as it solves the problem by using vectorization, avoiding the gradient descent iteration).\n",
    "This method is called Normal Equations or OLS - Ordinary least squares.\n",
    "<br>\n",
    "<br>\n",
    "**The formula to calculate the optimized array of thetas is as follows:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/NormalEquations.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data Again\n",
    "ex2_file = pd.read_csv('ex1data2.txt', header=None) \n",
    "X = np.array(ex2_file.iloc[:,0:2])\n",
    "y = np.array(ex2_file.iloc[:,2])\n",
    "\n",
    "m = y.size\n",
    "\n",
    "X, mu, sigma = featureNormalize(X)\n",
    "\n",
    "# Add column of ones to the array \n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(\n",
    "    X: np.array,\n",
    "    y: np.array\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes the optimal thetas using the OLS\n",
    "    method.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): The values of each observation.\n",
    "        y (np.array): The values to predict.\n",
    "        \n",
    "    Returns:\n",
    "        Theta (np.array): The array of optimized thetas returned by \n",
    "        the ordinary least square method (Normal equations).\n",
    "        \n",
    "    \"\"\" \n",
    "    # Placeholder for thetas\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    # OLS method calculation\n",
    "    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = normalEqn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from the Normal Equation Method \n",
      " [340412.65957447 110631.05027885  -6649.47427082]\n"
     ]
    }
   ],
   "source": [
    "print('Theta computed from the Normal Equation Method \\n {}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the thetas calculated by OLS is approximately the same as the ones returned by gradient descent. \n",
    "<br>\n",
    "And we did it with fewer lines of code! \n",
    "<br>\n",
    "**The caveat of this method is that it only works for linear equations - a limitation in terms of most of the functions that we want to learn - not many problems in the real world behave in a linear way.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have the same prediction for the new house? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for a 1650 sq-ft, 3 br house (using normal equation) is : \n",
      " 293081 $\n"
     ]
    }
   ],
   "source": [
    "test_example = np.array((1, 1650, 3)).astype('float64')\n",
    "test_example[1:] = (test_example[1:].reshape(1,2)-mu.reshape(1,2))/sigma.reshape(1,2)\n",
    "price = np.dot(test_example, theta)\n",
    "\n",
    "print ('Predicted price for a 1650 sq-ft, 3 br house (using normal equation) is : \\n {} $'.format(int(price)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
