{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os seguintes scripts são uma versão Python do curso de Machine Learning do professor Andrew Ng da universidade de Stanford leccionado na plataforma Coursera.\n",
    "\n",
    "**Nota: Todos os dados e estrutura do exercício pertencem à Universidade de Stanford**\n",
    "\n",
    "**Ressalva:** Os scripts não estão implementados de forma modular para todas as funções serem consultadas no mesmo Jupyter Notebook - ao contrário da implementação Octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar o numpy para lidar com matrizes e vectores\n",
    "import numpy as np\n",
    "# Importar o pandas para ler os ficheiros \n",
    "import pandas as pd\n",
    "# Importar matplotlib para construir gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro exercício consiste em desenvolver uma regressão linear com múltiplas variáveis.\n",
    "<br>\n",
    "<br>\n",
    "Quando lidamos com múltiplas variáveis, em muitos algoritmos, é necessário estandardizar as variáveis, ou seja, coloca-las na mesma escala. No algoritmo do gradiente descendente, normalizar as variáveis faz com que o algoritmo seja mais rápido a converger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1 - Normalizar as variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados que vamos analisar são de uma empresa de imobiliário. Neste exemplo, vamos tentar prever o valor de uma casa com base em duas variáveis:\n",
    "- O tamanho de casa (em metros quadrados);\n",
    "- O número de quartos;\n",
    "<br>\n",
    "<br>\n",
    "Conseguimos desenvolver um algoritmo capaz de, dado o tamanho da casa e o número de quartos, prever o preço da propriedade com precisão?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# exemplos de treino: 47 \n",
      "\n",
      "Alguns exemplos de treino da tabela: \n",
      "\n",
      "X = [1600    3] , y = 329900\n",
      "X = [2400    3] , y = 369000\n",
      "X = [1416    2] , y = 232000\n",
      "X = [3000    4] , y = 539900\n",
      "X = [1985    4] , y = 299900\n",
      "X = [1534    3] , y = 314900\n",
      "X = [1427    3] , y = 198999\n",
      "X = [1380    3] , y = 212000\n",
      "X = [1494    3] , y = 242500\n"
     ]
    }
   ],
   "source": [
    "# Ler o ficheiro de texto e criar o objecto das variáveis e o target y \n",
    "ex2_file = pd.read_csv('ex1data2.txt', header=None) \n",
    "\n",
    "X = np.array(ex2_file.iloc[:,0:2])\n",
    "\n",
    "y = np.array(ex2_file.iloc[:,2])\n",
    "\n",
    "print('# exemplos de treino: {} \\n'.format(len(X)))\n",
    "\n",
    "print('Alguns exemplos de treino da tabela: \\n')\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    print('X = {} , y = {}'.format(X[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar as variáveis - fórmula de estandardização\n",
    "\n",
    "def normalizacaoVariaveis(\n",
    "    variaveis: np.array\n",
    ") -> [np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Retorna a versão estandardizada da matriz \"variaveis\"\n",
    "    A fórmula aplicada para z é ((xi-média)/desvio padrão)\n",
    "    onde i é cada observação para a coluna nth\n",
    "    \n",
    "    Args:\n",
    "        variaveis (np.array): A matriz de n colunas para normalizar.\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (np.array): Matriz de valores estandardizados.\n",
    "        media (np.array): Media de cada coluna n.\n",
    "        desvpad (np.array): Desvio padrão de cada coluna n.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    media = variaveis.mean(axis=0)\n",
    "    # para ter o mesmo resultado de estandardização do Octave, usamos os graus de liberdade = 1\n",
    "    desvpad = variaveis.std(axis=0,ddof=1)\n",
    "    X_norm = ((variaveis-media)/desvpad)\n",
    "    return X_norm, media, desvpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aplicar a estandardização de acordo com a seguinte fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/Standardization.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A estandardização da variável Z é feita aplicando a fórmula acima:**\n",
    "<br>\n",
    "- Xi corresponde a inésima observação.\n",
    "<br>\n",
    "- μ é a média da variável.\n",
    "<br>\n",
    "- σ é o desvio padrão da variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a matriz normalizada, o vector de médias e o vector de desvios-padrão\n",
    "X, media, sigma = normalizacaoVariaveis(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar a coluna de uns \n",
    "vector_uns = np.ones((len(X),1))\n",
    "# Criamos um objecto X_ext para não reescrever o objecto X\n",
    "X_ext = np.hstack((vector_uns,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como fizemos com uma variável, vamos usar o gradiente descendente para encontrar os thetas óptimos que minimizam o erro.\n",
    "<br>\n",
    "A diferença é que agora vamos ter um novo Θ para optimizar (Θ2) - em vez de termos apenas Θ0 e Θ1.\n",
    "\n",
    "Vamos seguir uma abordagem vectorizada. Para grandes valores de m (observações) ou n (número de colunas+1, thetas para optimizar), uma abordagem vectorizada é preferencial sobre uma abordagem de ciclo já que é computacionalmente mais barata.\n",
    "\n",
    "Verificando as formulas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/Multivariate Cost Function.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2 - Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do gradiente para multiplas variáveis\n",
    "# Inicializar o vector de thetas, o rácio de aprendizagem e o número de iteracções\n",
    "theta = np.zeros((3,1))\n",
    "alpha = 0.1;\n",
    "num_iters = 400;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função de custo multi-variável\n",
    "def calcularCustoMulti (\n",
    "    X: np.array, \n",
    "    y:np.array , \n",
    "    theta: np.array\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    \n",
    "    Retorna a função de custo (média do erro quadrado)\n",
    "    usando o cálculo da matriz X*theta e o seu output Y\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Os valores de cada observação.\n",
    "        y (np.array): Os valores a prever.\n",
    "        theta (np.array): O peso dado a cada variável \n",
    "        que será calculado por X e representa a hipótese.\n",
    "        \n",
    "    Retorna:\n",
    "        J (int): A soma dos erros quadrados \n",
    "        - total da função de custo\n",
    "        \n",
    "    \"\"\" \n",
    "    m = len(y) \n",
    "    # Calcular hipótese\n",
    "    hipotese = np.dot(X,theta)\n",
    "    # Calcular a função de custo, usando o erro quadrado\n",
    "    J = (sum((hipotese-y.reshape(len(y),1))**2))/(2*m)\n",
    "    return J\n",
    "\n",
    "# Definir a função do gradiente multivariável\n",
    "def gradientDescentMulti(\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    theta: np.array,\n",
    "    alpha: float,\n",
    "    num_iters: int\n",
    ") -> [np.array, np.array]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Calcula o theta óptimo baseado nas iterações \n",
    "    e histórico da função de custo.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Os valores de cada observação.\n",
    "        y (np.array): Os valores de previsão.\n",
    "        theta (np.array): O peso inicial a ser atríbuido a cada\n",
    "        variável para o cálculo da hipótese.\n",
    "        alpha(float): O rácio de aprendizagem do gradiente descendente.\n",
    "        num_iters(int): O número de iterações.\n",
    "        \n",
    "    Retorna:\n",
    "        Theta (np.array): O vector de thetas optimizado retornado\n",
    "        pela última iteração do gradiente descendente.\n",
    "        J_history (np.array): A história do erro ao longo\n",
    "        das diversas iterações do gradiente descendente.\n",
    "        \n",
    "    \"\"\" \n",
    "    \n",
    "    m = len(y)\n",
    "    # Inicializar o vector de histórico das funções de custo\n",
    "    J_historico = np.zeros((num_iters, 1))\n",
    "    # Iterar por cada iteracao\n",
    "    for i in np.arange(0,num_iters):\n",
    "        # Calcular a hipótese\n",
    "        hipotese = np.dot(X,theta)\n",
    "        # Update theta by computing gradient\n",
    "        theta -= alpha * (1/m) * np.dot(np.transpose(X),(hipotese-y.reshape(len(y),1)))\n",
    "        # Assignar a função de custo ao element i do histórico\n",
    "        J_historico[i] = calcularCustoMulti(X,y,theta)\n",
    "    return theta, J_historico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returnar o valor do theta óptimo computado pelo gradiente descendente e respectivo histórico.\n",
    "theta, J_historico = gradientDescentMulti(X_ext,y,theta,alpha,num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao colocarmos num gráfico a convergência da função de custo, notamos alguns dados:\n",
    "<br>\n",
    "    -  A função de custo estabiliza depois de ~25 iterações;\n",
    "    -  A função de custa muda mais rápido nas primeiras iterações. Depois da 25 iteração, a mudança nos parâmetros Θ apenas trazem mudanças décimas na função de custo J;\n",
    "<br>\n",
    "Vamos usar o código seguinte para colocar num gráfico o histórico de funções de custo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Função de Custo - J - Erro Quadrado Médio')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Colocar o valor de J ao longo das iterações do gradiente descendente\n",
    "plt.plot(np.arange(0,num_iters),J_historico,c='blue')\n",
    "plt.xlabel('Número de Iterações')\n",
    "plt.ylabel('Função de Custo - J - Erro Quadrado Médio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A função de custo pode convergir para um mínimo mais lentamente se mudarmos um rácio de aprendizagem menor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qual foi o valor óptimo de thetas gerado?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta calculado pelo gradiente descendente:\n",
      " [[340412.65957447]\n",
      " [110631.04895815]\n",
      " [ -6649.47295013]]\n"
     ]
    }
   ],
   "source": [
    "print('Theta calculado pelo gradiente descendente:\\n {}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos agora usar os novos thetas para prever exemplos de novas casas. Por exemplo, imaginemos que temos uma nova casa que tem de área 1650 metros quadrados e tem três quartos. Dado isto, qual o preço previsto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = np.array((1, 1650, 3)).astype('float64')\n",
    "test_example[1:] = (test_example[1:].reshape(1,2)-media.reshape(1,2))/sigma.reshape(1,2)\n",
    "preco = np.dot(test_example, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preço previsto para uma casa de 1650 m^2 e três quartos (usando o gradiente descendente): \n",
      " 293081 $\n"
     ]
    }
   ],
   "source": [
    "print ('Preço previsto para uma casa de 1650 m^2 e três quartos (usando o gradiente descendente): \\n {} $'.format(int(preco)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este problema linear, usamos o gradiente descendente para minizar a função de erro e calcularmos os nossos thetas optimizados.\n",
    "<br>\n",
    "<br> \n",
    "Mas, existe outro métodos que podemos utilizar para encontrar o erro mínimo numa regressão linear (este método acaba por também ser computacionalmente melhor já que recorre exclusivamente a vectorizações, evitando o gradiente descendente).\n",
    "Este método chama-se de Método dos Minimos Quadrados.\n",
    "<br>\n",
    "<br>\n",
    "**A fórmula utilizada para calcular o vector otimizado de thetas usando o MMQ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/NormalEquations.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 4 - MMQ - Metódo Mínimos Quadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler os dados novamente\n",
    "ex2_file = pd.read_csv('ex1data2.txt', header=None) \n",
    "X = np.array(ex2_file.iloc[:,0:2])\n",
    "y = np.array(ex2_file.iloc[:,2])\n",
    "\n",
    "m = y.size\n",
    "\n",
    "# Estandardizar\n",
    "X, _, _ = normalizacaoVariaveis(X)\n",
    "\n",
    "# Adicionar uma coluna de 1's \n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodoMinimosQuadrados(\n",
    "    X: np.array,\n",
    "    y: np.array\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    \n",
    "    Calculo os thetas optimos utilizado o \n",
    "    Método dos mínimos quadrados.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Os valores para cada observação.\n",
    "        y (np.array): Os valores a prever.\n",
    "        \n",
    "    Retorna:\n",
    "        Theta (np.array): O vector de thetas optimizado\n",
    "        pelo método dos mínimos quadrados.\n",
    "        \n",
    "    \"\"\" \n",
    "    # Inicializar vector de thetas\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    # Calcular thetas\n",
    "    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = metodoMinimosQuadrados(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os thetas calculados pelo método dos mínimos quadrados:\n",
      " [340412.65957447 110631.05027885  -6649.47427082]\n"
     ]
    }
   ],
   "source": [
    "print('Os thetas calculados pelo método dos mínimos quadrados:\\n {}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparamos como os thetas calculados pelo MMQ é aproximadamente o mesmo que foi retornado pela optimização do gradiente descendente. \n",
    "<br>\n",
    "E conseguimos fazer isso com muito menos linhas de código! \n",
    "<br>\n",
    "**A ressalva neste método é que apenas funciona para equações lineares - uma enorme limitação no que a funções que queremos aprender com aprendizagem de máquina - não são muitos os problemas no mundo real que podem ser aprendidos com uma simples função linear.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos a mesma previsão para a nova casa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preço previsto para uma casa de 1650 m^2 e três quartos (usando o método dos mínimos quadrados): \n",
      " 293081 $\n"
     ]
    }
   ],
   "source": [
    "exemplo_teste = np.array((1, 1650, 3)).astype('float64')\n",
    "exemplo_teste[1:] = (exemplo_teste[1:].reshape(1,2)-media.reshape(1,2))/sigma.reshape(1,2)\n",
    "preco = np.dot(exemplo_teste, theta)\n",
    "\n",
    "print ('Preço previsto para uma casa de 1650 m^2 e três quartos (usando o método dos mínimos quadrados): \\n {} $'.format(int(preco)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
